<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Yunhao Gou's Homepage">


<title>Yunhao Gou's Homepage</title>


<link rel="stylesheet" href="./css/bootstrap.min.css">
<link rel="stylesheet" href="./css/all.min.css">
<link rel="stylesheet" href="./css/academicons.min.css">
<link rel="stylesheet" href="./css/charts.min.css">
<link id="theme-style" rel="stylesheet" href="./css/main.css">

<!-- <link rel="icon" type="image/png" href="./images/favicon.png"> -->

<script type="module" src="./css/background_star.js"></script>

<style type="text/css">.dg ul{list-style:none;margin:0;padding:0;width:100%;clear:both}.dg.ac{position:fixed;top:0;left:0;right:0;height:0;z-index:0}.dg:not(.ac) .main{overflow:hidden}.dg.main{-webkit-transition:opacity .1s linear;-o-transition:opacity .1s linear;-moz-transition:opacity .1s linear;transition:opacity .1s linear}.dg.main.taller-than-window{overflow-y:auto}.dg.main.taller-than-window .close-button{opacity:1;margin-top:-1px;border-top:1px solid #2c2c2c}.dg.main ul.closed .close-button{opacity:1 !important}.dg.main:hover .close-button,.dg.main .close-button.drag{opacity:1}.dg.main .close-button{-webkit-transition:opacity .1s linear;-o-transition:opacity .1s linear;-moz-transition:opacity .1s linear;transition:opacity .1s linear;border:0;line-height:19px;height:20px;cursor:pointer;text-align:center;background-color:#000}.dg.main .close-button.close-top{position:relative}.dg.main .close-button.close-bottom{position:absolute}.dg.main .close-button:hover{background-color:#111}.dg.a{float:right;margin-right:15px;overflow-y:visible}.dg.a.has-save>ul.close-top{margin-top:0}.dg.a.has-save>ul.close-bottom{margin-top:27px}.dg.a.has-save>ul.closed{margin-top:0}.dg.a .save-row{top:0;z-index:1002}.dg.a .save-row.close-top{position:relative}.dg.a .save-row.close-bottom{position:fixed}.dg li{-webkit-transition:height .1s ease-out;-o-transition:height .1s ease-out;-moz-transition:height .1s ease-out;transition:height .1s ease-out;-webkit-transition:overflow .1s linear;-o-transition:overflow .1s linear;-moz-transition:overflow .1s linear;transition:overflow .1s linear}.dg li:not(.folder){cursor:auto;height:27px;line-height:27px;padding:0 4px 0 5px}.dg li.folder{padding:0;border-left:4px solid rgba(0,0,0,0)}.dg li.title{cursor:pointer;margin-left:-4px}.dg .closed li:not(.title),.dg .closed ul li,.dg .closed ul li>*{height:0;overflow:hidden;border:0}.dg .cr{clear:both;padding-left:3px;height:27px;overflow:hidden}.dg .property-name{cursor:default;float:left;clear:left;width:40%;overflow:hidden;text-overflow:ellipsis}.dg .c{float:left;width:60%;position:relative}.dg .c input[type=text]{border:0;margin-top:4px;padding:3px;width:100%;float:right}.dg .has-slider input[type=text]{width:30%;margin-left:0}.dg .slider{float:left;width:66%;margin-left:-5px;margin-right:0;height:19px;margin-top:4px}.dg .slider-fg{height:100%}.dg .c input[type=checkbox]{margin-top:7px}.dg .c select{margin-top:5px}.dg .cr.function,.dg .cr.function .property-name,.dg .cr.function *,.dg .cr.boolean,.dg .cr.boolean *{cursor:pointer}.dg .cr.color{overflow:visible}.dg .selector{display:none;position:absolute;margin-left:-9px;margin-top:23px;z-index:10}.dg .c:hover .selector,.dg .selector.drag{display:block}.dg li.save-row{padding:0}.dg li.save-row .button{display:inline-block;padding:0px 6px}.dg.dialogue{background-color:#222;width:460px;padding:15px;font-size:13px;line-height:15px}#dg-new-constructor{padding:10px;color:#222;font-family:Monaco, monospace;font-size:10px;border:0;resize:none;box-shadow:inset 1px 1px 1px #888;word-wrap:break-word;margin:12px 0;display:block;width:440px;overflow-y:scroll;height:100px;position:relative}#dg-local-explain{display:none;font-size:11px;line-height:17px;border-radius:3px;background-color:#333;padding:8px;margin-top:10px}#dg-local-explain code{font-size:10px}#dat-gui-save-locally{display:none}.dg{color:#eee;font:11px 'Lucida Grande', sans-serif;text-shadow:0 -1px 0 #111}.dg.main::-webkit-scrollbar{width:5px;background:#1a1a1a}.dg.main::-webkit-scrollbar-corner{height:0;display:none}.dg.main::-webkit-scrollbar-thumb{border-radius:5px;background:#676767}.dg li:not(.folder){background:#1a1a1a;border-bottom:1px solid #2c2c2c}.dg li.save-row{line-height:25px;background:#dad5cb;border:0}.dg li.save-row select{margin-left:5px;width:108px}.dg li.save-row .button{margin-left:5px;margin-top:1px;border-radius:2px;font-size:9px;line-height:7px;padding:4px 4px 5px 4px;background:#c5bdad;color:#fff;text-shadow:0 1px 0 #b0a58f;box-shadow:0 -1px 0 #b0a58f;cursor:pointer}.dg li.save-row .button.gears{background:#c5bdad url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAANCAYAAAB/9ZQ7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAQJJREFUeNpiYKAU/P//PwGIC/ApCABiBSAW+I8AClAcgKxQ4T9hoMAEUrxx2QSGN6+egDX+/vWT4e7N82AMYoPAx/evwWoYoSYbACX2s7KxCxzcsezDh3evFoDEBYTEEqycggWAzA9AuUSQQgeYPa9fPv6/YWm/Acx5IPb7ty/fw+QZblw67vDs8R0YHyQhgObx+yAJkBqmG5dPPDh1aPOGR/eugW0G4vlIoTIfyFcA+QekhhHJhPdQxbiAIguMBTQZrPD7108M6roWYDFQiIAAv6Aow/1bFwXgis+f2LUAynwoIaNcz8XNx3Dl7MEJUDGQpx9gtQ8YCueB+D26OECAAQDadt7e46D42QAAAABJRU5ErkJggg==) 2px 1px no-repeat;height:7px;width:8px}.dg li.save-row .button:hover{background-color:#bab19e;box-shadow:0 -1px 0 #b0a58f}.dg li.folder{border-bottom:0}.dg li.title{padding-left:16px;background:#000 url(data:image/gif;base64,R0lGODlhBQAFAJEAAP////Pz8////////yH5BAEAAAIALAAAAAAFAAUAAAIIlI+hKgFxoCgAOw==) 6px 10px no-repeat;cursor:pointer;border-bottom:1px solid rgba(255,255,255,0.2)}.dg .closed li.title{background-image:url(data:image/gif;base64,R0lGODlhBQAFAJEAAP////Pz8////////yH5BAEAAAIALAAAAAAFAAUAAAIIlGIWqMCbWAEAOw==)}.dg .cr.boolean{border-left:3px solid #806787}.dg .cr.color{border-left:3px solid}.dg .cr.function{border-left:3px solid #e61d5f}.dg .cr.number{border-left:3px solid #2FA1D6}.dg .cr.number input[type=text]{color:#2FA1D6}.dg .cr.string{border-left:3px solid #1ed36f}.dg .cr.string input[type=text]{color:#1ed36f}.dg .cr.function:hover,.dg .cr.boolean:hover{background:#111}.dg .c input[type=text]{background:#303030;outline:none}.dg .c input[type=text]:hover{background:#3c3c3c}.dg .c input[type=text]:focus{background:#494949;color:#fff}.dg .c .slider{background:#303030;cursor:ew-resize}.dg .c .slider-fg{background:#2FA1D6;max-width:100%}.dg .c .slider:hover{background:#3c3c3c}.dg .c .slider:hover .slider-fg{background:#44abda}
</style>

<style type="text/css">.dg ul{list-style:none;margin:0;padding:0;width:100%;clear:both}.dg.ac{position:fixed;top:0;left:0;right:0;height:0;z-index:0}.dg:not(.ac) .main{overflow:hidden}.dg.main{-webkit-transition:opacity .1s linear;-o-transition:opacity .1s linear;-moz-transition:opacity .1s linear;transition:opacity .1s linear}.dg.main.taller-than-window{overflow-y:auto}.dg.main.taller-than-window .close-button{opacity:1;margin-top:-1px;border-top:1px solid #2c2c2c}.dg.main ul.closed .close-button{opacity:1 !important}.dg.main:hover .close-button,.dg.main .close-button.drag{opacity:1}.dg.main .close-button{-webkit-transition:opacity .1s linear;-o-transition:opacity .1s linear;-moz-transition:opacity .1s linear;transition:opacity .1s linear;border:0;line-height:19px;height:20px;cursor:pointer;text-align:center;background-color:#000}.dg.main .close-button.close-top{position:relative}.dg.main .close-button.close-bottom{position:absolute}.dg.main .close-button:hover{background-color:#111}.dg.a{float:right;margin-right:15px;overflow-y:visible}.dg.a.has-save>ul.close-top{margin-top:0}.dg.a.has-save>ul.close-bottom{margin-top:27px}.dg.a.has-save>ul.closed{margin-top:0}.dg.a .save-row{top:0;z-index:1002}.dg.a .save-row.close-top{position:relative}.dg.a .save-row.close-bottom{position:fixed}.dg li{-webkit-transition:height .1s ease-out;-o-transition:height .1s ease-out;-moz-transition:height .1s ease-out;transition:height .1s ease-out;-webkit-transition:overflow .1s linear;-o-transition:overflow .1s linear;-moz-transition:overflow .1s linear;transition:overflow .1s linear}.dg li:not(.folder){cursor:auto;height:27px;line-height:27px;padding:0 4px 0 5px}.dg li.folder{padding:0;border-left:4px solid rgba(0,0,0,0)}.dg li.title{cursor:pointer;margin-left:-4px}.dg .closed li:not(.title),.dg .closed ul li,.dg .closed ul li>*{height:0;overflow:hidden;border:0}.dg .cr{clear:both;padding-left:3px;height:27px;overflow:hidden}.dg .property-name{cursor:default;float:left;clear:left;width:40%;overflow:hidden;text-overflow:ellipsis}.dg .c{float:left;width:60%;position:relative}.dg .c input[type=text]{border:0;margin-top:4px;padding:3px;width:100%;float:right}.dg .has-slider input[type=text]{width:30%;margin-left:0}.dg .slider{float:left;width:66%;margin-left:-5px;margin-right:0;height:19px;margin-top:4px}.dg .slider-fg{height:100%}.dg .c input[type=checkbox]{margin-top:7px}.dg .c select{margin-top:5px}.dg .cr.function,.dg .cr.function .property-name,.dg .cr.function *,.dg .cr.boolean,.dg .cr.boolean *{cursor:pointer}.dg .cr.color{overflow:visible}.dg .selector{display:none;position:absolute;margin-left:-9px;margin-top:23px;z-index:10}.dg .c:hover .selector,.dg .selector.drag{display:block}.dg li.save-row{padding:0}.dg li.save-row .button{display:inline-block;padding:0px 6px}.dg.dialogue{background-color:#222;width:460px;padding:15px;font-size:13px;line-height:15px}#dg-new-constructor{padding:10px;color:#222;font-family:Monaco, monospace;font-size:10px;border:0;resize:none;box-shadow:inset 1px 1px 1px #888;word-wrap:break-word;margin:12px 0;display:block;width:440px;overflow-y:scroll;height:100px;position:relative}#dg-local-explain{display:none;font-size:11px;line-height:17px;border-radius:3px;background-color:#333;padding:8px;margin-top:10px}#dg-local-explain code{font-size:10px}#dat-gui-save-locally{display:none}.dg{color:#eee;font:11px 'Lucida Grande', sans-serif;text-shadow:0 -1px 0 #111}.dg.main::-webkit-scrollbar{width:5px;background:#1a1a1a}.dg.main::-webkit-scrollbar-corner{height:0;display:none}.dg.main::-webkit-scrollbar-thumb{border-radius:5px;background:#676767}.dg li:not(.folder){background:#1a1a1a;border-bottom:1px solid #2c2c2c}.dg li.save-row{line-height:25px;background:#dad5cb;border:0}.dg li.save-row select{margin-left:5px;width:108px}.dg li.save-row .button{margin-left:5px;margin-top:1px;border-radius:2px;font-size:9px;line-height:7px;padding:4px 4px 5px 4px;background:#c5bdad;color:#fff;text-shadow:0 1px 0 #b0a58f;box-shadow:0 -1px 0 #b0a58f;cursor:pointer}.dg li.save-row .button.gears{background:#c5bdad url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAANCAYAAAB/9ZQ7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAQJJREFUeNpiYKAU/P//PwGIC/ApCABiBSAW+I8AClAcgKxQ4T9hoMAEUrxx2QSGN6+egDX+/vWT4e7N82AMYoPAx/evwWoYoSYbACX2s7KxCxzcsezDh3evFoDEBYTEEqycggWAzA9AuUSQQgeYPa9fPv6/YWm/Acx5IPb7ty/fw+QZblw67vDs8R0YHyQhgObx+yAJkBqmG5dPPDh1aPOGR/eugW0G4vlIoTIfyFcA+QekhhHJhPdQxbiAIguMBTQZrPD7108M6roWYDFQiIAAv6Aow/1bFwXgis+f2LUAynwoIaNcz8XNx3Dl7MEJUDGQpx9gtQ8YCueB+D26OECAAQDadt7e46D42QAAAABJRU5ErkJggg==) 2px 1px no-repeat;height:7px;width:8px}.dg li.save-row .button:hover{background-color:#bab19e;box-shadow:0 -1px 0 #b0a58f}.dg li.folder{border-bottom:0}.dg li.title{padding-left:16px;background:#000 url(data:image/gif;base64,R0lGODlhBQAFAJEAAP////Pz8////////yH5BAEAAAIALAAAAAAFAAUAAAIIlI+hKgFxoCgAOw==) 6px 10px no-repeat;cursor:pointer;border-bottom:1px solid rgba(255,255,255,0.2)}.dg .closed li.title{background-image:url(data:image/gif;base64,R0lGODlhBQAFAJEAAP////Pz8////////yH5BAEAAAIALAAAAAAFAAUAAAIIlGIWqMCbWAEAOw==)}.dg .cr.boolean{border-left:3px solid #806787}.dg .cr.color{border-left:3px solid}.dg .cr.function{border-left:3px solid #e61d5f}.dg .cr.number{border-left:3px solid #2FA1D6}.dg .cr.number input[type=text]{color:#2FA1D6}.dg .cr.string{border-left:3px solid #1ed36f}.dg .cr.string input[type=text]{color:#1ed36f}.dg .cr.function:hover,.dg .cr.boolean:hover{background:#111}.dg .c input[type=text]{background:#303030;outline:none}.dg .c input[type=text]:hover{background:#3c3c3c}.dg .c input[type=text]:focus{background:#494949;color:#fff}.dg .c .slider{background:#303030;cursor:ew-resize}.dg .c .slider-fg{background:#2FA1D6;max-width:100%}.dg .c .slider:hover{background:#3c3c3c}.dg .c .slider:hover .slider-fg{background:#44abda}
</style>

<style>.crx_bdwk_down_wrap {
    top: 70%;
    left: 0;
    position: fixed;
    z-index: 99999999;
    color: #fff;
    user-select: none;
}

    .crx_bdwk_down_wrap .crx_bdwk_down_loading {
        background-color: #666;
        cursor: wait;
        width: 126px;
        text-align: center;
        padding: 16px 0;
    }

    .crx_bdwk_down_wrap .crx_bdwk_down_loading p {
            font-size: 14px;
        }

    .crx_bdwk_down_wrap .crx_bdwk_down_loading small {
            font-size: 10px;
        }

    .crx_bdwk_down_wrap .crx_bdwk_down_btn {
        width: 126px;
        height: 60px;
        display: flex;
        align-items: center;
        justify-content: center;
        cursor: pointer;
        font-size: 14px;
        background-color: #dd5a57;
        position: relative;
    }

    .crx_bdwk_down_wrap .crx_bdwk_down_types {
        display: flex;
        text-align: center;
        align-items: center;
        background-color: #666;
        font-size: 12px;
    }

    .crx_bdwk_down_wrap .crx_bdwk_down_types div {
            position: relative;
        }

    .crx_bdwk_down_wrap .crx_bdwk_down_types div:after {
                content: ' ';
                height: 12px;
                width: 1px;
                background-color: #eee;
                position: absolute;
                top: 10px;
                right: 0;
                transform: scaleX(0.5);
            }

    .crx_bdwk_down_wrap .crx_bdwk_down_types div:last-child:after {
                    display: none;
                }

    .crx_bdwk_down_wrap .crx_bdwk_down_types_check {
            flex: 1;
            color: #dd5a57;
            padding: 8px;
            cursor: pointer;
            font-weight: bold;
        }

    .crx_bdwk_down_wrap .crx_bdwk_down_types_uncheck {
            flex: 1;
            padding: 8px;
            cursor: pointer;
            color: #fff;
            font-weight: lighter;
        }
</style>
</head>

<body>
<div class="wrapper">
<section class="section intro-section">
    <div class="avatar-container">
        <img class="avatar" src="./images/profile.jpg" alt="Kai Chen avatar">
    </div>
    <div class="intro-container">
        <div class="header">
            <h2 class="name">Yunhao Gou (苟耘豪)</h2>
            <h3 class="title">Ph.D. Candidate @ HKUST</h3>
        </div>
        <div class="contact">
            <a class="link" href="mailto:ygou@connect.ust.hk" target="_blank">Email</a>
                &nbsp/&nbsp
            <a class="link" href="./files/CV_of_Yunhao.pdf" target="_blank">CV</a>
                &nbsp/&nbsp
            <a class="link" href="https://github.com/gyhdog99" target="_blank">Github</a>
                &nbsp/&nbsp
            <a class="link" href="https://scholar.google.com/citations?user=RYDHIccAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>
        </div>
    </div>
</section>

<!-- Introduction -->
<section class="section profile-section">
    <div class="section-title">
        About Me
    </div>
    <div class="details">
        <p> I am currently a Ph.D. candidate in CSE department of Hong Kong University of Science and Technology (HKUST), supervised jointly by <a href="https://www.cse.ust.hk/~jamesk/" target="_blank">Prof. James T. Kwok</a> and <a href="https://yuzhanghk.github.io/" target="_blank">Prof. Yu Zhang</a>. 
        Previously, I was an undergraduate student majoring in Software Engineering in University of Electronic Science and Technology of China (UESTC).
        </p>

        <p>
        My current research interests include:
        <ul>
            <li><strong>Post-training of MLLMs/LLMs: </strong> <a href="https://arxiv.org/abs/2502.12635" target="_blank">Corrupted but not Broken</a>, <a href="https://arxiv.org/abs/2409.18042" target="_blank">EMOVA</a>, <a href="https://arxiv.org/abs/2403.09572" target="_blank">ECSO</a>, <a href="https://arxiv.org/abs/2405.00557" target="_blank">MoTE</a></li>
            <li><strong>New paradigms of MLLMs: </strong> <a href="https://arxiv.org/abs/2506.04559" target="_blank">RACRO</a>, <a href="https://gyhdog99.github.io/projects/mocle/" target="_blank">MoCLE</a> </li>
            <li><strong>Vision and Language Representation Learning: </strong> <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.html" target="_blank">EPIC</a>, <a href="https://kaiyi.me/p/hgrnet" target="_blank">HGR-Net</a>, <a href="https://arxiv.org/abs/2110.07130" target="_blank">RSAN</a>   </li> 
            
            
        </ul>
        </p>
    </div>
</section>

<!-- News -->
<section class="section news-section">
    <div class="section-title">
        News
    </div>
    <div style="height:200px;overflow-y:auto">
    <ul>
        <li> [2025.05] One paper (<a href="https://arxiv.org/abs/2405.00557">MoTE</a>) accepted by ACL 2025! See you in Vienna!</li>
        <li> [2025.02] One paper (<a href="https://emova-ollm.github.io/">EMOVA</a>) accepted by CVPR 2025! See you in Nashville!</li>
        <li> [2025.02] One paper (<a href="https://arxiv.org/abs/2403.09572/">ECSO</a>) accepted by ECCV 2024! See you in Milano!</li>
        <li> [2024.03] Code and checkpoints of <a href="https://gyhdog99.github.io/projects/mocle/" target="_blank">MoCLE</a> and <a href="https://arxiv.org/abs/2403.09572" target="_blank">ECSO</a> have been released. Welcome to try! </li>
        <li> [2024.03] Our work <a href="https://arxiv.org/abs/2403.09572" target="_blank">ECSO</a>, the first work that makes MLLM safe without neither training nor any external models, is on <a href="https://arxiv.org/abs/2403.09572" target="_blank">Arxiv</a>!</li>
        <li> [2023.12] Our work <a href="https://gyhdog99.github.io/projects/mocle/" target="_blank">MoCLE</a> is reported by <a href="https://mp.weixin.qq.com/s/JSB0wNhU1GBRKJOEZbMHZw?poc_token=HNDKj2Wjaz666RfdxhJtszL19BQwHusWeK3MNPWI" target="_blank">QbitAI</a></li>
        <li> [2023.12] Our work <a href="https://gyhdog99.github.io/projects/mocle/" target="_blank">MoCLE</a>, the first MLLM with MoE architecture for instruction customization and generalization, is on <a href="https://arxiv.org/abs/2312.12379" target="_blank">Arxiv</a>!</li>
        <li> [2023.02] One paper accepted by CVPR 2023!</li>
        <li> [2022.09] Join <a href="https://cse.hkust.edu.hk/" target="_blank">HKUST CSE</a> for PhD study.</li>
        <li> [2022.01] Joint <a href="https://ailab.bytedance.com/" target="_blank">Bytedance AILab</a>  as an intern researcher.</li>
        <li> [2022.07] One paper accepted by ECCV 2022! </li>
        <li> [2021.07] One paper accepted by CIKM 2021! </li>
    </ul></div>
</section>

<!-- Publications -->
<section class="section publications-section">
    <div class="section-title">
        Selected Publications
    </div>
    <p>Full publication list on <a class="link" href="https://scholar.google.com/citations?user=RYDHIccAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>. (* denotes equal contribution)</p>  

    <div class="publication-container">
        <div class="teaser-container">
        <div class="teaser-container">
            <img class="teaser" style="margin-top: 25px;" src="https://github.com/gyhdog99/RACRO2/raw/main/assets/images/intro.png" alt="racro.png">
        </div>
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2506.04559" target="_blank">Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning</a>
                </p>
                <div class="authors"><p><strong>Yunhao Gou*</strong>, Kai Chen*, Zhili Liu*, Lanqing Hong, Xin Jin, Zhenguo Li, James T. Kwok, Yu Zhang</p></div>
                <div class="authors"><p><em><b><span style="color:red">Scaling reasoning MLLMs via adopting any advanced LLM reasoners during inference time!</span></em></p></b></div>
                <div class="conference"><p><em>Arxiv preprint, 2025</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2506.04559" target="_blank">[PDF]</a>
                <a class="url" href="https://huggingface.co/spaces/Emova-ollm/RACRO-demo" target="_blank">[Demo]</a>
                <a class="url" href="https://github.com/gyhdog99/RACRO2" target="_blank">[Code]</a>
                <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gyhdog99/RACRO2">
        </div>
    </div>

    <div class="publication-container">
        <div class="teaser-container">
            <img class="teaser" src="./images/pub/mote.png" alt="mote.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2405.00557" target="_blank">Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment</a>
                </p>
                <div class="authors"><p>Zhili Liu*, <strong>Yunhao Gou*</strong>, Kai Chen*, Lanqing Hong, Jiahui Gao, Fei Mi, Yu Zhang, Zhenguo Li, Xin Jiang, Qun Liu, James T. Kwok</p></div>
                <!-- <div class="authors"><p><em><b><span style="color:red">Self-alignment with MoE-empowered CoT multi-dimensional analysis!</span></em></p></b></div> -->
                <div class="conference"><p><em>Annual Meeting of the Association for Computational Linguistics (ACL), 2025.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2405.00557" target="_blank">[PDF]</a>
        </div>
    </div>
    
   <div class="publication-container">
        <div class="teaser-container">
            <img class="teaser" style="margin-top: 10px;" src="./images/pub/val-ppl.png" alt="val-ppl.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2502.12635" target="_blank">Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning</a>
                </p>
                <div class="authors"><p><strong>Yunhao Gou*</strong>, Hansi Yang*, Zhili Liu, Kai Chen, Yihan Zeng, Lanqing Hong, Zhenguo Li, Qun Liu, James T Kwok, Yu Zhang.</p></div>
                <div class="conference"><p><em>Arxiv preprint, 2025.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2502.12635" target="_blank">[PDF]</a>
        </div>
    </div>


    <div class="publication-container">
    <div class="teaser-container">
        <video class="teaser" autoplay controls muted loop src="https://emova-ollm.github.io/static/images/video_demo_compressed.mp4" alt="emova.mp4">
    </div>
    <div class="info-container">
            <p class="title">
                <a class="title_link" href="https://arxiv.org/abs/2409.18042" target="_blank">EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</a>
            </p>
            <div class="authors"><p>Kai Chen*, <strong>Yunhao Gou*</strong>, Runhui Huang*, Zhili Liu*, Daxin Tan* <em>and other 26 authors</em></p></div>
            <div class="authors"><p><em><b><span style="color:red">Fully open-sourced Omni-modal LLMs with SoTA vision-language and speech abilities!</span></em></p></b></div>
            <div class="conference"><p><em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2025</em></p></div>
            <a class="url" href="https://arxiv.org/abs/2409.18042" target="_blank">[PDF]</a>
            <a class="url" href="https://emova-ollm.github.io/" target="_blank">[Webpage]</a>
            <a class="url" href="https://www.youtube.com/watch?v=wtI2RLyCQNc" target="_blank">[Talk]</a>
            <a class="url" href="https://www.bilibili.com/video/BV1VsTfzdEJ8/" target="_blank">[Talk (Chinese)]</a>
            <a class="url" href="https://mp.weixin.qq.com/s/e2KkDjqbWNy7wSv0geCNUg" target="_blank">[Wechat Post]</a>
            <a class="url" href="https://github.com/emova-ollm/EMOVA" target="_blank">[Code]</a>
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emova-ollm/EMOVA">
    </div>
</div>

    <div class="publication-container">
        <div class="teaser-container">
            <img class="teaser" src="./images/pub/ecso.png" alt="ecso.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2403.09572" target="_blank">Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</a>
                </p>
                <div class="authors"><p> <strong>Yunhao Gou*</strong>, Kai Chen*, Zhili Liu*, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok, Yu Zhang.</p></div>
                <div class="authors">
                    <p><em><b><span style="color:red">1) Make MLLM safe without neither training nor any external models!</span></b></em></p>
                    <p><em><b><span style="color:red">2) Free data engine for MLLM alignment on its own!</span></b></em></p>
                </div>
                <div class="conference"><p><em>European Conference on Computer Vision (ECCV), 2024.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2403.09572" target="_blank">[PDF]</a>
                <a class="url" href="https://gyhdog99.github.io/projects/ecso/" target="_blank">[Project page]</a>
        </div>
    </div>

    <div class="publication-container">
        <div class="teaser-container">
                
            <img class="teaser" src="./images/pub/mocle.png" alt="mocle.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2312.12379" target="_blank">Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning</a>
                </p>
                <div class="authors"><p> <strong>Yunhao Gou*</strong>, Zhili Liu*, Kai Chen*, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James Kwok, Yu Zhang.</p></div>
                <div class="authors"><p><em><b><span style="color:red">First MLLM with MoE for instruction customization and generalization!</span></em></p></b></div>
                <div class="conference"><p><em>Arxiv preprint, 2023.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2312.12379" target="_blank">[PDF]</a>
                <a class="url" href="https://gyhdog99.github.io/projects/mocle/" target="_blank">[Project page]</a>
                <a class="url" href="https://mp.weixin.qq.com/s/JSB0wNhU1GBRKJOEZbMHZw?poc_token=HNDKj2Wjaz666RfdxhJtszL19BQwHusWeK3MNPWI" target="_blank">[Wechat Post]</a>
                <a class="url" href="https://www.techbeat.net/talk-info?id=849" target="_blank">[Talk]</a>
        </div>
    </div>

    <div class="publication-container">
        <div class="teaser-container">
                
            <img class="teaser" src="./images/pub/epic.png" alt="epic.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://openaccess.thecvf.com/content/CVPR2023/html/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.html" target="_blank">Leveraging per Image-Token Consistency for Vision-Language Pre-training</a>
                </p>
                <div class="authors"><p> <strong>Yunhao Gou</strong>, Tom Ko, Hansi Yang, James Kwok, Yu Zhang, Mingxuan Wang.</p></div>
                <div class="conference"><p><em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2023.</em></p></div>
                <a class="url" href="https://openaccess.thecvf.com/content/CVPR2023/html/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.html" target="_blank">[PDF]</a>
        </div>
    </div>

    <div class="publication-container">
        <div class="teaser-container">
                
            <img class="teaser" src="./images/pub/hgrnet.png" alt="hgrnet.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2203.01386" target="_blank">Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification</a>
                </p>
                <div class="authors"><p> Kai Yi, Xiaoqian Shen, <strong>Yunhao Gou</strong>, Mohamed Elhoseiny. </p></div>
                <div class="conference"><p><em>European Conference on Computer Vision (ECCV), 2022.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2203.01386" target="_blank">[PDF]</a>
                <a class="url" href="https://kaiyi.me/p/hgrnet" target="_blank">[Project page]</a>
        </div>
    </div>

    <div class="publication-container">
        <div class="teaser-container">
                
            <img class="teaser" src="./images/pub/rsan.png" alt="rsan.png">
        </div>
        <div class="info-container">
                <p class="title">
                    <a class="title_link" href="https://arxiv.org/abs/2110.07130" target="_blank">Region semantically aligned network for zero-shot learning</a>
                </p>
                <div class="authors"><p> Ziyang Wang*, <strong>Yunhao Gou*</strong>, Jingjing Li, Yu Zhang, Yang Yang </p></div>
                <div class="conference"><p><em>International Conference on Information & Knowledge Management (CIKM), 2021.</em></p></div>
                <a class="url" href="https://arxiv.org/abs/2110.07130" target="_blank">[PDF]</a>
        </div>
    </div>


</section>

<section class="section awards-section">
    <div class="section-title">
        Academic Services
    </div>
    <div class="item">
        <div class="upper-row">
            <div class="name">
                Reviewer: 
                    <ul><li> Conference: NeurIPS 2025, EMNLP 2025 (ARR May), ACL 2025 (ARR Feb), ICML 2025, ECCV 2022, AAAI 2024.</li>
                    </ul>
            </div>
        </div>
    </div>
</section>


<!-- Talks -->
<section class="section awards-section">
    <div class="section-title">
        Talks
    </div>
    <div class="item">
        <div class="upper-row">
            <div class="name">
                <ul>
                <li> [TechBeat Online] Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning. <a class="url" href="https://www.bilibili.com/video/BV1o4421c73H/?spm_id_from=333.788.recommend_more_video.12" target="_blank">[Recording]</a></li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Experiences -->
<section class="section experiences-section">
    <div class="section-title">
        Experiences
    </div>
    <div class="item">

        <div class="upper-row">
            <div class="company">
                 National University of Singapore
            </div>
            <div class="time">July 2019 - Aug. 2019</div>
        </div>
        <div class="below-row">
            <div class="role">International exchange student
            </div>
        </div>

        <div class="upper-row">
            <div class="company">
                 King Abdullah University of Science and Technology 
            </div>
            <div class="time">July. 2021 - Dec. 2021</div>
        </div>
        <div class="below-row">
            <div class="role">Visiting student, working with Prof. <a class="url" href="http://www.mohamed-elhoseiny.com" target="_blank">Mohammed Elhoseiny</a>
            </div>
        </div>

        <div class="upper-row">
            <div class="company">
                 ByteDance AI Lab
            </div>
            <div class="time">Jan. 2022 - Mar. 2023</div>
        </div>
        <div class="below-row">
            <div class="role">Research Intern, working with <a class="url" href="https://tomkocse.github.io/" target="_blank">Tom Ko</a>
            </div>
        </div>

        <div class="upper-row">
            <div class="company">
                 Huawei Noah’s Ark Lab (AI Theory group)
            </div>
            <div class="time">Oct. 2023 - Now</div>
        </div>
        <div class="below-row">
            <div class="role">Research Intern, working with <a class="url" href="https://racheltechie.github.io/" target="_blank">Lanqing Hong</a>
            </div>
        </div>


    </div>
</section>


<!-- Awards -->
<section class="section awards-section">
    <div class="section-title">
        Selected Awards
    </div>

    <div class="item">
        <div class="upper-row">
            <div class="name">
                <p>Research Travel Grant HKUST</p>
            </div>
            <div class="time">2023</div>
        </div>
    </div>

    <div class="item">
        <div class="upper-row">
            <div class="name">
                <p>Postgraduate Scholarship HKUST</p>
            </div>
            <div class="time">2022</div>
        </div>
    </div>

    <div class="item">
        <div class="upper-row">
            <div class="name">
                <p>Oversea Visiting Student Stipend of UESTC</p>
            </div>
            <div class="time">2019</div>
        </div>
    </div>

    <div class="item">
        <div class="upper-row">
            <div class="name">
                <p>National Scholarship</p>
            </div>
            <div class="time">2019</div>
        </div>
    </div>
    
</section>


<footer class="footer">
    <div class="text-center">
        <small id="copyright">
            @ Yunhao Gou
        </small>
    </div>
</footer>


</body></html>
